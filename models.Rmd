---
title: Model Fitting on Abortion Rates
output: html_document
---

# Questions for Alex:

- need to study what the p-values mean in model summary vs moran test again

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}

knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

# packages

{
  library(MASS)
  library(tidyverse)
  library(raster)
  library(spdep)
  library(spatialreg)
  select <- dplyr::select
  library(ggpubr)
  library(knitr)
  library(kableExtra)
}

# data

combined <- read_csv('raw-data/combined_data.csv') %>% 
  inner_join(
    read.delim('https://www2.census.gov/geo/docs/reference/cenpop2020/CenPop2020_Mean_ST.txt') %>% 
      separate(STATEFP.STNAME.POPULATION.LATITUDE.LONGITUDE, sep = ',',
               into = c('statefip', 'state', 'population', 'latitude', 'longitude')) %>% 
      mutate(across(c(latitude, longitude), as.numeric)) %>% 
      select(state, latitude, longitude), by = 'state')

# getting df that drops observations with NA in predictors/outcomes

nbin_df <- combined %>% 
  drop_na(abortions, births, intrastate_score, interstate_score,
          pct_bachelors, total_population, prop_hisp, prop_nonwhite,
          hh_income, dem_2party)

```

## Neighbors and Weights

```{r spatial_weights}

{
  {
    # getting spatial data for US state boundaries & subsetting out Alaska & Hawaii
    # (using tutorial at https://mhallwor.github.io/_pages/basics_SpatialPolygons)

    usa <- raster::getData('GADM', country='USA', level=1)
    usa <- usa[!usa$NAME_1 %in% c('Alaska', 'Hawaii'),]

    # merging with data

    usa <- merge(usa, nbin_df,
                 by.x = 'NAME_1', by.y = 'state',
                 duplicateGeoms = TRUE, all.x = FALSE)
    
    # ordering within-between categories so that the reference grouping makes sense
    
    usa@data$within_between <- fct_relevel(usa@data$within_between,
                                       'low-low', 'med-low', 'high-low',
                                       'low-med', 'med-med', 'high-med',
                                       'low-high', 'med-high', 'high-high')
    
  }

  # using poly2nb to create a neighbors list & from that a neighbors matrix
  # https://rspatial.org/raster/rosu/Chapter7.html

  ### ******* I MAY WANT TO ADD A SNAP ARGUMENT IN THE FUTURE TO CONSIDER BOUNDARY
  ### POINTS LESS THAN SNAP DISTANCE APART AS NEIGHBORS... STATES THAT DO NOT
  ### SHARE BOUNDARIES ARE STILL CLOSE TO ONE ANOTHER AND HAVE REASONABLE
  ### SPILLOVER / REGIONAL CORRELATION **********
  # maybe specify neighbors with dnearneigh instead?

  # usa_contig <- poly2nb(usa, queen=FALSE)

}

# writing the nb object to a file so that I don't have to recreate it every time
# (it takes forever) & can just read it in directly

# write.nb.gal(usa_contig, 'raw-data/helper/usa_contig_nb.gal')
usa_contig <- read.gal('raw-data/helper/usa_contig_nb.gal')

# check for symmetric relationships (since if TX is a neighbor of OK, OK should
# also be a neighbor of TX)

is.symmetric.nb(usa_contig)

# build binary weight matrix

weights.contig.B <- nb2listw(usa_contig, style = "B")
print(weights.contig.B)

# rescaling each row of B to sum to 1

weights.contig.W <- nb2listw(usa_contig)
image(listw2mat(weights.contig.W)[,281:1],
      axes = FALSE)

```

```{r dist_matrix}

# creating weights based on the inverse distance

inv.dist <- lapply(nbdists(usa.dist.range, coordinates(usa)),
                   function(x) ifelse(x!=0, 1e3/(x), x))
weights.inv.dist <- nb2listw(usa.dist.range, 
                             glist = inv.dist, # specify weights
                             style = "B")

# note style B uses the weights specified by inv.dist, 
# setting style = W will renormalize the weights to sum to one

print(weights.inv.dist)
image(listw2mat(weights.inv.dist)[,281:1],
      axes = FALSE)

# also created weights based on inverse distance squared, which will give a
# greater penalty to states with larger distances (similar to the construction
# of the interstate scores)

inv.dist2 <- lapply(nbdists(usa.dist.range, coordinates(usa)),
                   function(x) ifelse(x!=0, 1e3/(x^2), x))
weights.inv.dist2 <- nb2listw(usa.dist.range, 
                              glist = inv.dist2, # specify weights
                              style = "B")

# note style B uses the weights specified by inv.dist, 
# setting style = W will renormalize the weights to sum to one

print(weights.inv.dist2)
image(listw2mat(weights.inv.dist2)[,281:1],
      axes = FALSE)

```

## Testing for Spatial Autocorrelation

** need to figure out what this actually means for the data **

```{r global_moran}

# moran test on row-normalized weights for contiguous neighbors

moran.test(usa$abortion_per_1k_births,
           listw = weights.contig.W)

# moran test using the inverse weighted distance & distance squared

moran.test(usa$abortion_per_1k_births,
           listw = weights.inv.dist)
moran.test(usa$abortion_per_1k_births,
           listw = weights.inv.dist2)

```

## Model fitting

```{r rate_mod_function}

methods <- c('lm', 'wlm', 'car', 'wcar', 'sar', 'wsar')
weights_matrix <- weights.inv.dist

# writing a function to fit all of the models

make_models <- function(weights_matrix) {
  
  # creating each model object

  rate_lm <- lm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year),
              data = usa@data)
  rate_lmw <- lm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year), 
                 data = usa@data, 
                 weights = births)
  rate_car <- spautolm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_wcar <- spautolm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_sar <- spautolm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 family = 'SAR',
                 listw = weights_matrix)
  rate_wsar <- spautolm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'SAR',
                 listw = weights_matrix)
  models <- list(rate_lm, rate_lmw, rate_car, rate_wcar, rate_sar, rate_wsar)
  
  
  
  morans <- vector('list', length = length(models))
  plots <- vector('list', length = length(models))
  
  for (i in 1:length(models)) {
    
    ################
    #### MORAN
    ################
    
    {
      # conducting a Moran test on each of the models, assessing whether (p < 0.05) or
      # not (p >= 0.05) we have evidence of spatial autocorrelation not captured by
      # the model...
      # using built-in function for our lm objects, but it doesn't work on spautolm
      # objects
      
      if (class(models[[i]]) == 'lm') {
        morans[[i]] <- lm.morantest(models[[i]], listw = weights_matrix, alternative = 'two.sided')
      }
      
      # conduct a Moran test of the residuals, where p < 0.05 provides sufficient
      # evidence of additional autocorrelation not captured by the model
      
      else {
        morans[[i]] <- moran.test(residuals(models[[i]]), listw = weights_matrix, alternative = 'two.sided')
      }
    }
    
    ################
    #### PLOTS
    ################
    
    {
      # generating diagnostic plots for each model
  
      resid_vs_fit <- tibble(fit = fitted(models[[i]]), 
             resid = residuals(models[[i]])) %>% 
        ggplot(aes(fit, resid)) +
        geom_point() +
        geom_hline(yintercept = 0,
                   color = 'red',
                   linetype = 'dashed') +
        theme_minimal() +
        labs(title = 'Residuals vs. fitted values',
             x = 'Fitted values',
             y = 'Residuals')
      plot_data <- tibble(fit = fitted(models[[i]]), 
                          resid = residuals(models[[i]])) 
      qqplot <- plot_data %>% 
        ggplot(aes(sample = resid)) +
        stat_qq() +
        stat_qq_line(col = 'red') +
        theme_minimal() +
        labs(title = 'QQ-plot for normality of residuals',
             x = 'Theoretical quantiles',
             y = 'Sample quantiles')
      
      plots[[i]] <- annotate_figure(ggarrange(resid_vs_fit, qqplot),
                                    top = text_grob(paste('Diagnosic plots for', methods[[i]]),size = 14))
    }
  }
  
  # I want to return the methods, models, moran tests, and diagnostic plots
  
  return_values <- list(methods, models, morans, plots)
  return(return_values)

}

# generating the models and diagnostics for the two different weights matrices

rates_inv_dist <- make_models(weights.inv.dist)
rates_inv_dist2 <- make_models(weights.inv.dist2)
rates_contig <- make_models(weights.contig.W)

get_metrics_table(rates_inv_dist)

rates_inv_dist[[2]][3]

```

### OLS

#### SHOULD I SQRT THE Y-VALUES? even though it loses its easy interpretability & the difference in the distributions is pretty negligible

```{r distributions}

#I want to plot all of the dependent variables & their respective
#transformations to Gaussian

{
  # this requires some data manipulation, which I'm doing below
  
  untransformed <- usa@data %>% 
    select(abortion_per_1k_births,
           ie_ratio,
           nonres_res_ratio,
           late_to_early) %>% 
    pivot_longer(everything()) %>% 
    mutate(transformed = FALSE,
           transformation = case_when(name == 'abortion_per_1k_births' ~ 'log(x)',
                                      name == 'ie_ratio' ~ 'log(x)',
                                      name == 'nonres_res_ratio' ~ 'log(x - 1)',
                                      name == 'late_to_early' ~ 'none')) 
  
  graph_df <- untransformed %>% 
    mutate(value = case_when(name == 'abortion_per_1k_births' ~ log(value),
                                      name == 'ie_ratio' ~ log(value),
                                      name == 'nonres_res_ratio' ~ log(value-1),
                                      name == 'late_to_early' ~ value),
           transformed = TRUE) %>% 
    bind_rows(untransformed) %>% 
    mutate(transformation = ifelse(transformed, transformation, 'untransformed')) 
  
  # creating a list of plots in a loop to streamline the code
  
  plots <- list()
  dep_vars <- c('abortion_per_1k_births', 'ie_ratio', 'nonres_res_ratio', 'late_to_early')
  for (i in 1:length(dep_vars)) {
    plots[[i]] <- graph_df %>% 
      filter(name == dep_vars[i]) %>% 
      mutate(transformation = fct_relevel(transformation, 'untransformed')) %>% 
      ggplot(aes(value)) +
      geom_histogram() +
      facet_wrap(~transformation, scales = 'free') +
      labs(title = dep_vars[[i]], x = '', y = '') +
      theme_minimal()
  }
  
  # printing out the plots
  
  ggarrange(plots[[1]], plots[[2]], plots[[3]], plots[[4]]) %>% 
    annotate_figure(left = textGrob('Count', rot = 90, vjust = 1),
                    bottom = textGrob('Value'),
                    top = text_grob('Dependent variables as approximately Gaussian distributions', 
                                    size = 16)) 

}

# plot to justify including year as a factor rather than linear term in the models

usa@data %>% 
  select(year, abortion_per_1k_births, ie_ratio, 
         nonres_res_ratio, late_to_early) %>% 
  pivot_longer(2:ncol(.), names_to = 'variable') %>% 
  group_by(year, variable) %>% 
  summarise(avg_rate = mean(value), 
            .groups = 'drop') %>% 
  ggplot(aes(year, avg_rate)) +
  geom_point() +
  geom_line() +
  facet_wrap(~variable, scales = 'free_y') + 
  theme_minimal() +
  labs(title = 'Non-linear nature of year across variables') +
  scale_x_continuous(breaks = 2010:2019)

```

#### INTERPRET MORAN TEST


```{r rate_lm}

# printing model output from the function that created all of the models

summary(rates_inv_dist[[2]][1][[1]])

# printing moran test

rates_inv_dist[[3]][1][[1]]

# printing diagnostic plots

rates_inv_dist[[4]][1][[1]]

```

```{r rate_wlm}

# printing model output from the function that created all of the models

summary(rates_inv_dist[[2]][2][[1]])

# printing moran test

rates_inv_dist[[3]][2][[1]]

# printing diagnostic plots

rates_inv_dist[[4]][2][[1]]

```

```{r rate_car}

# printing model output from the function that created all of the models

summary(rates_inv_dist[[2]][3][[1]])

# printing moran test

rates_inv_dist[[3]][3][[1]]

# printing diagnostic plots

rates_inv_dist[[4]][3][[1]]

```

```{r rate_wcar}

# printing model output from the function that created all of the models

summary(rates_inv_dist[[2]][4][[1]])

# printing moran test

rates_inv_dist[[3]][4][[1]]

# printing diagnostic plots

rates_inv_dist[[4]][4][[1]]

```

```{r rate_sar}

# printing model output from the function that created all of the models

summary(rates_inv_dist[[2]][5][[1]])

# printing moran test

rates_inv_dist[[3]][5][[1]]

# printing diagnostic plots

rates_inv_dist[[4]][5][[1]]

```

```{r rate_wsar}

# printing model output from the function that created all of the models

summary(rates_inv_dist[[2]][6][[1]])

# printing moran test

rates_inv_dist[[3]][6][[1]]

# printing diagnostic plots

rates_inv_dist[[4]][6][[1]]

```

## Model Comparisons

```{r rate_comparisons}

# creating table to compare fit of various models

get_metrics <- function(mod) {
  if (is.null(summary(mod)$lambda)) {
    metrics <- c(AIC(mod), logLik(mod), NA, NA)
  }
  else {
    metrics <- c(AIC(mod), logLik(mod), summary(mod)$lambda, summary(mod)$lambda.se)
  }
  return(metrics)
}

metrics <- c('AIC', 'logLik', 'lambda', 'lambda se')

get_metrics_table <- function(model_list) {
  tibble(method = rep(methods, each = length(metrics)),
       metric = rep(metrics, times = length(methods)),
       value = unlist(lapply(model_list[[2]], get_metrics))) %>% 
    pivot_wider(names_from = metric, values_from = value)
}

get_metrics_table(rates_inv_dist) %>% 
  mutate(weights = 'inv_dist') %>% 
  bind_rows(get_metrics_table(rates_inv_dist2) %>% 
              mutate(weights = 'inv_dist2')) %>% 
  bind_rows(get_metrics_table(rates_contig) %>% 
              mutate(weights = 'contig')) %>% 
  select(method, AIC, weights) %>% 
  pivot_wider(names_from = weights,
              values_from = AIC,
              names_prefix = 'AIC_') %>% 
  mutate(best_weights = case_when(AIC_inv_dist < AIC_inv_dist2 & AIC_inv_dist < AIC_contig ~ 'dist',
                                  AIC_inv_dist > AIC_inv_dist2 & AIC_inv_dist2 < AIC_contig ~ 'dist2',
                                  AIC_contig < AIC_inv_dist2 & AIC_inv_dist > AIC_contig ~ 'contig',
                                  TRUE ~ ''),
         best_weights = ifelse(best_weights == '', NA, best_weights)) %>% 
  kable(caption = 'Model selection for abortion rates',
        booktabs = TRUE)

```

## Inference on SAR Model

 I need to graph the confidence intervals for each of the within-between coefficients
(or I should think of other ways to visualize the within-between coefficients that also display which ones are significant)

```{r sar_inference}

# printing moran test.. both this & the LR in the model summary have p-values <
# 0.05... figure out what that means again

rates_inv_dist[[3]][5][[1]]

# printing diagnostic plots

rates_inv_dist[[4]][5][[1]]

# printing out coefficients for policy categories on our chosen model

rate_sar <- rates_inv_dist[[2]][5][[1]]
summary(rate_sar)
data.frame(coef(rate_sar)) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  arrange(coef.rate_sar.)

```

# Other dependent variables

## Import-export ratio

```{r ie_mod_function}

methods <- c('lm', 'wlm', 'car', 'wcar', 'sar', 'wsar')
weights_matrix <- weights.inv.dist

# writing a function to fit all of the models

make_models <- function(weights_matrix) {
  
  # creating each model object

  rate_lm <- lm(log(ie_ratio) ~ within_between + pct_bachelors + prop_hisp + 
                       prop_nonwhite + hh_income + dem_2party + as.factor(year),
              data = usa@data)
  rate_lmw <- lm(log(ie_ratio) ~ within_between + pct_bachelors + prop_hisp + 
                       prop_nonwhite + hh_income + dem_2party + as.factor(year), 
                 data = usa@data, 
                 weights = births)
  rate_car <- spautolm(log(ie_ratio) ~ within_between + pct_bachelors + prop_hisp + 
                       prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_wcar <- spautolm(log(ie_ratio) ~ within_between + pct_bachelors + prop_hisp + 
                       prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_sar <- spautolm(log(ie_ratio) ~ within_between + pct_bachelors + prop_hisp + 
                       prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 family = 'SAR',
                 listw = weights_matrix)
  rate_wsar <- spautolm(log(ie_ratio) ~ within_between + pct_bachelors + prop_hisp + 
                       prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'SAR',
                 listw = weights_matrix)
  models <- list(rate_lm, rate_lmw, rate_car, rate_wcar, rate_sar, rate_wsar)
  
  
  
  morans <- vector('list', length = length(models))
  plots <- vector('list', length = length(models))
  
  for (i in 1:length(models)) {
    
    ################
    #### MORAN
    ################
    
    {
      # conducting a Moran test on each of the models, assessing whether (p < 0.05) or
      # not (p >= 0.05) we have evidence of spatial autocorrelation not captured by
      # the model...
      # using built-in function for our lm objects, but it doesn't work on spautolm
      # objects
      
      if (class(models[[i]]) == 'lm') {
        morans[[i]] <- lm.morantest(models[[i]], listw = weights_matrix, alternative = 'two.sided')
      }
      
      # conduct a Moran test of the residuals, where p < 0.05 provides sufficient
      # evidence of additional autocorrelation not captured by the model
      
      else {
        morans[[i]] <- moran.test(residuals(models[[i]]), listw = weights_matrix, alternative = 'two.sided')
      }
    }
    
    ################
    #### PLOTS
    ################
    
    {
      # generating diagnostic plots for each model
  
      resid_vs_fit <- tibble(fit = fitted(models[[i]]), 
             resid = residuals(models[[i]])) %>% 
        ggplot(aes(fit, resid)) +
        geom_point() +
        geom_hline(yintercept = 0,
                   color = 'red',
                   linetype = 'dashed') +
        theme_minimal() +
        labs(title = 'Residuals vs. fitted values',
             x = 'Fitted values',
             y = 'Residuals')
      plot_data <- tibble(fit = fitted(models[[i]]), 
                          resid = residuals(models[[i]])) 
      qqplot <- plot_data %>% 
        ggplot(aes(sample = resid)) +
        stat_qq() +
        stat_qq_line(col = 'red') +
        theme_minimal() +
        labs(title = 'QQ-plot for normality of residuals',
             x = 'Theoretical quantiles',
             y = 'Sample quantiles')
      
      plots[[i]] <- annotate_figure(ggarrange(resid_vs_fit, qqplot),
                                    top = text_grob(paste('Diagnosic plots for', methods[[i]]),size = 14))
    }
  }
  
  # I want to return the methods, models, moran tests, and diagnostic plots
  
  return_values <- list(methods, models, morans, plots)
  return(return_values)

}

# generating the models and diagnostics for the two different weights matrices

ie_inv_dist <- make_models(weights.inv.dist)
ie_inv_dist2 <- make_models(weights.inv.dist2)
ie_contig <- make_models(weights.contig.W)

```

```{r ie_comparisons}

# producing table as I did before... lowest AIC observed in SAR model with
# inv_dist weights

get_metrics_table(ie_inv_dist) %>% 
  mutate(weights = 'inv_dist') %>% 
  bind_rows(get_metrics_table(ie_inv_dist2) %>% 
              mutate(weights = 'inv_dist2')) %>% 
  bind_rows(get_metrics_table(ie_contig) %>% 
              mutate(weights = 'contig')) %>% 
  select(method, AIC, weights) %>% 
  pivot_wider(names_from = weights,
              values_from = AIC,
              names_prefix = 'AIC_') %>% 
  mutate(best_weights = case_when(AIC_inv_dist < AIC_inv_dist2 & AIC_inv_dist < AIC_contig ~ 'dist',
                                  AIC_inv_dist > AIC_inv_dist2 & AIC_inv_dist2 < AIC_contig ~ 'dist2',
                                  AIC_contig < AIC_inv_dist2 & AIC_inv_dist > AIC_contig ~ 'contig',
                                  TRUE ~ ''),
         best_weights = ifelse(best_weights == '', NA, best_weights)) %>% 
  kable(caption = 'Model selection for import-export ratios',
        booktabs = TRUE)

```

```{r ie_sar}

# printing out coefficients for policy categories on our chosen model

ie_sar <- ie_inv_dist[[2]][5][[1]]
summary(ie_sar)
data.frame(coef(ie_sar)) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  arrange(coef.ie_sar.)

# printing moran test.. both this & the LR in the model summary have p-values <
# 0.05... figure out what that means again

ie_inv_dist[[3]][5][[1]]

# printing diagnostic plots

ie_inv_dist[[4]][5][[1]]

# getting coefficients ready to interpret... higher IE ratio indicates that more
# people travel in the state and few people leave the state.... lower ratios
# indicate that more people leave the state and fewer people come into the state
# for abortions... having the largest decrease in the IE ratio would then
# indicate that it corresponds to more people seeking abortions in neighboring
# states relative to the state in question

data.frame(coef(ie_sar)) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  mutate(coef.ie_sar. = exp(coef.ie_sar.)) %>% 
  arrange(coef.ie_sar.) %>% 
  rename(exp_coef = coef.ie_sar.)

```


## Nonresident-resident ratio

```{r nonres_mod_function}

# transforming the variable so that it's approximately normally distributed

hist(usa@data$nonres_res_ratio)
hist(log(usa@data$nonres_res_ratio - 1))

# copied code from above functions but changed the model terms to match the
# nonres-res specification

methods <- c('lm', 'wlm', 'car', 'wcar', 'sar', 'wsar')
weights_matrix <- weights.inv.dist

# writing a function to fit all of the models

make_models <- function(weights_matrix) {
  
  # creating each model object

  rate_lm <- lm(log(nonres_res_ratio-1) ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
              data = usa@data)
  rate_lmw <- lm(log(nonres_res_ratio-1) ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year), 
                 data = usa@data, 
                 weights = births)
  rate_car <- spautolm(log(nonres_res_ratio-1) ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
                 data = usa@data,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_wcar <- spautolm(log(nonres_res_ratio-1) ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_sar <- spautolm(log(nonres_res_ratio-1) ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
                 data = usa@data,
                 family = 'SAR',
                 listw = weights_matrix)
  rate_wsar <- spautolm(log(nonres_res_ratio-1) ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'SAR',
                 listw = weights_matrix)
  models <- list(rate_lm, rate_lmw, rate_car, rate_wcar, rate_sar, rate_wsar)
  
  
  
  morans <- vector('list', length = length(models))
  plots <- vector('list', length = length(models))
  
  for (i in 1:length(models)) {
    
    ################
    #### MORAN
    ################
    
    {
      # conducting a Moran test on each of the models, assessing whether (p < 0.05) or
      # not (p >= 0.05) we have evidence of spatial autocorrelation not captured by
      # the model...
      # using built-in function for our lm objects, but it doesn't work on spautolm
      # objects
      
      if (class(models[[i]]) == 'lm') {
        morans[[i]] <- lm.morantest(models[[i]], listw = weights_matrix, alternative = 'two.sided')
      }
      
      # conduct a Moran test of the residuals, where p < 0.05 provides sufficient
      # evidence of additional autocorrelation not captured by the model
      
      else {
        morans[[i]] <- moran.test(residuals(models[[i]]), listw = weights_matrix, alternative = 'two.sided')
      }
    }
    
    ################
    #### PLOTS
    ################
    
    {
      # generating diagnostic plots for each model
  
      resid_vs_fit <- tibble(fit = fitted(models[[i]]), 
             resid = residuals(models[[i]])) %>% 
        ggplot(aes(fit, resid)) +
        geom_point() +
        geom_hline(yintercept = 0,
                   color = 'red',
                   linetype = 'dashed') +
        theme_minimal() +
        labs(title = 'Residuals vs. fitted values',
             x = 'Fitted values',
             y = 'Residuals')
      plot_data <- tibble(fit = fitted(models[[i]]), 
                          resid = residuals(models[[i]])) 
      qqplot <- plot_data %>% 
        ggplot(aes(sample = resid)) +
        stat_qq() +
        stat_qq_line(col = 'red') +
        theme_minimal() +
        labs(title = 'QQ-plot for normality of residuals',
             x = 'Theoretical quantiles',
             y = 'Sample quantiles')
      
      plots[[i]] <- annotate_figure(ggarrange(resid_vs_fit, qqplot),
                                    top = text_grob(paste('Diagnosic plots for', methods[[i]]),size = 14))
    }
  }
  
  # I want to return the methods, models, moran tests, and diagnostic plots
  
  return_values <- list(methods, models, morans, plots)
  return(return_values)

}

# generating the models and diagnostics for the two different weights matrices

nonres_res_inv_dist <- make_models(weights.inv.dist)
nonres_res_inv_dist2 <- make_models(weights.inv.dist2)
nonres_res_contig <- make_models(weights.contig.W)

```

```{r nonres_res_comparisons}

# producing table as I did before... lowest AIC observed in SAR model with
# inv_dist weights

get_metrics_table(nonres_res_inv_dist) %>% 
  mutate(weights = 'inv_dist') %>% 
  bind_rows(get_metrics_table(nonres_res_inv_dist2) %>% 
              mutate(weights = 'inv_dist2')) %>% 
  bind_rows(get_metrics_table(nonres_res_contig) %>% 
              mutate(weights = 'contig')) %>% 
  select(method, AIC, weights) %>% 
  pivot_wider(names_from = weights,
              values_from = AIC,
              names_prefix = 'AIC_') %>% 
  mutate(best_weights = case_when(AIC_inv_dist < AIC_inv_dist2 & AIC_inv_dist < AIC_contig ~ 'dist',
                                  AIC_inv_dist > AIC_inv_dist2 & AIC_inv_dist2 < AIC_contig ~ 'dist2',
                                  AIC_contig < AIC_inv_dist2 & AIC_inv_dist > AIC_contig ~ 'contig',
                                  TRUE ~ ''),
         best_weights = ifelse(best_weights == '', NA, best_weights)) %>% 
  kable(caption = 'Model selection for nonresident-to-resident ratios',
        booktabs = TRUE)

```

```{r nonres_res_sar}

# printing out coefficients for policy categories on our chosen model

nonres_res_sar <- nonres_res_inv_dist[[2]][5][[1]]
summary(nonres_res_sar)
data.frame(coef(nonres_res_sar)) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  arrange(coef.nonres_res_sar.)

# printing moran test.. both this & the LR in the model summary have p-values <
# 0.05... figure out what that means again

nonres_res_inv_dist[[3]][5][[1]]

# printing diagnostic plots

nonres_res_inv_dist[[4]][5][[1]]

# log(nonres/res - 1) = log((nonres-res) / res), so exponentiating coefficients
# yields the multiplicative increase in the difference between nonresident and
# resident abortions as a ratio of resident abortions... larger values means
# that there are relatively more out-of-state abortions & smaller values mean
# that there are relatively fewer out-of-state abortions

data.frame(coef(nonres_res_sar)) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  mutate(coef.nonres_res_sar. = exp(coef.nonres_res_sar.)) %>% 
  arrange(desc(coef.nonres_res_sar.)) %>% 
  rename(exp_coef = coef.nonres_res_sar.)

```

## Late-to-early ratio

```{r late_early_mod_function}

# try fitting this on the raw data & see what happens with the residuals
# (qqplot)... if residuals still look good, we can stick with using the original
# data, but if not we can use the square root

hist(usa@data$late_to_early)
hist(sqrt(usa@data$late_to_early))

# copied code from above functions but changed the model terms to match the
# nonres-res specification

methods <- c('lm', 'wlm', 'car', 'wcar', 'sar', 'wsar')
weights_matrix <- weights.inv.dist

# writing a function to fit all of the models

make_models <- function(weights_matrix) {
  
  # creating each model object

  rate_lm <- lm(late_to_early ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
              data = usa@data)
  rate_lmw <- lm(late_to_early ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year), 
                 data = usa@data, 
                 weights = births)
  rate_car <- spautolm(late_to_early ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
                 data = usa@data,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_wcar <- spautolm(late_to_early ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_sar <- spautolm(late_to_early ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
                 data = usa@data,
                 family = 'SAR',
                 listw = weights_matrix)
  rate_wsar <- spautolm(late_to_early ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'SAR',
                 listw = weights_matrix)
  models <- list(rate_lm, rate_lmw, rate_car, rate_wcar, rate_sar, rate_wsar)
  
  morans <- vector('list', length = length(models))
  plots <- vector('list', length = length(models))
  
  for (i in 1:length(models)) {
    
    ################
    #### MORAN
    ################
    
    {
      # conducting a Moran test on each of the models, assessing whether (p < 0.05) or
      # not (p >= 0.05) we have evidence of spatial autocorrelation not captured by
      # the model...
      # using built-in function for our lm objects, but it doesn't work on spautolm
      # objects
      
      if (class(models[[i]]) == 'lm') {
        morans[[i]] <- lm.morantest(models[[i]], listw = weights_matrix, alternative = 'two.sided')
      }
      
      # conduct a Moran test of the residuals, where p < 0.05 provides sufficient
      # evidence of additional autocorrelation not captured by the model
      
      else {
        morans[[i]] <- moran.test(residuals(models[[i]]), listw = weights_matrix, alternative = 'two.sided')
      }
    }
    
    ################
    #### PLOTS
    ################
    
    {
      # generating diagnostic plots for each model
  
      resid_vs_fit <- tibble(fit = fitted(models[[i]]), 
             resid = residuals(models[[i]])) %>% 
        ggplot(aes(fit, resid)) +
        geom_point() +
        geom_hline(yintercept = 0,
                   color = 'red',
                   linetype = 'dashed') +
        theme_minimal() +
        labs(title = 'Residuals vs. fitted values',
             x = 'Fitted values',
             y = 'Residuals')
      plot_data <- tibble(fit = fitted(models[[i]]), 
                          resid = residuals(models[[i]])) 
      qqplot <- plot_data %>% 
        ggplot(aes(sample = resid)) +
        stat_qq() +
        stat_qq_line(col = 'red') +
        theme_minimal() +
        labs(title = 'QQ-plot for normality of residuals',
             x = 'Theoretical quantiles',
             y = 'Sample quantiles')
      
      plots[[i]] <- annotate_figure(ggarrange(resid_vs_fit, qqplot),
                                    top = text_grob(paste('Diagnosic plots for', methods[[i]]),size = 14))
    }
  }
  
  # I want to return the methods, models, moran tests, and diagnostic plots
  
  return_values <- list(methods, models, morans, plots)
  return(return_values)

}

# generating the models and diagnostics for the two different weights matrices

late_early_inv_dist <- make_models(weights.inv.dist)
late_early_inv_dist2 <- make_models(weights.inv.dist2)
late_early_contig <- make_models(weights.contig.W)

```

```{r late_early_comparisons}

# producing table as I did before... lowest AIC observed in SAR model with
# inv_dist weights

get_metrics_table(late_early_inv_dist) %>% 
  mutate(weights = 'inv_dist') %>% 
  bind_rows(get_metrics_table(late_early_inv_dist2) %>% 
              mutate(weights = 'inv_dist2')) %>% 
  bind_rows(get_metrics_table(late_early_contig) %>% 
              mutate(weights = 'contig')) %>% 
  select(method, AIC, weights) %>% 
  pivot_wider(names_from = weights,
              values_from = AIC,
              names_prefix = 'AIC_') %>% 
  mutate(best_weights = case_when(AIC_inv_dist < AIC_inv_dist2 & AIC_inv_dist < AIC_contig ~ 'dist',
                                  AIC_inv_dist > AIC_inv_dist2 & AIC_inv_dist2 < AIC_contig ~ 'dist2',
                                  AIC_contig < AIC_inv_dist2 & AIC_inv_dist > AIC_contig ~ 'contig',
                                  TRUE ~ ''),
         best_weights = ifelse(best_weights == '', NA, best_weights)) %>% 
  kable(caption = 'Model selection for late-to-early ratios',
        booktabs = TRUE)

```

# are residuals good enough? or should I square-root transform while sacrificing interpretability? I also need to update the comments for this code about the coefficient interpretations

```{r late_early_sar}

# printing out coefficients for policy categories on our chosen model

late_early_sar <- late_early_inv_dist[[2]][5][[1]]
summary(late_early_sar)
data.frame(coef(late_early_sar)) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  arrange(coef.late_early_sar.)

# printing moran test.. both this & the LR in the model summary have p-values <
# 0.05... figure out what that means again

late_early_inv_dist[[3]][5][[1]]

# printing diagnostic plots

late_early_inv_dist[[4]][5][[1]]

# I currently have the raw late-to-early ratios, so larger coefficients indicate
# that there is more likely to be a relatively higher number of later-term
# abortions in states like these

data.frame(coef(late_early_sar)) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  mutate(coef.late_early_sar. = exp(coef.late_early_sar.)) %>% 
  arrange(desc(coef.late_early_sar.)) %>% 
  rename(exp_coef = coef.late_early_sar.)

```

