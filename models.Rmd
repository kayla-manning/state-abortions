---
title: Model Fitting on Abortion Rates
output: html_document
---

# Questions for Alex:

- how do I know that I computed the weights correctly?
  - try inverse distance squared

- distance-based weights make more sense I think? but that changes the p-values in Moran tests to determine whether or not additional spatial autocorrelation exists in the model that's not picked up... should I compute the models both ways and then select the best one based on AIC? and same goes for the scores vs score categories? (AIC for SAR was better with categories)

- need to study what the p-values mean in model summary vs moran test again

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}

knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

# packages

{
  library(MASS)
  library(tidyverse)
  library(raster)
  library(spdep)
  library(spatialreg)
  select <- dplyr::select
  library(ggpubr)
}

# data

combined <- read_csv('raw-data/combined_data.csv') %>% 
  inner_join(
    read.delim('https://www2.census.gov/geo/docs/reference/cenpop2020/CenPop2020_Mean_ST.txt') %>% 
      separate(STATEFP.STNAME.POPULATION.LATITUDE.LONGITUDE, sep = ',',
               into = c('statefip', 'state', 'population', 'latitude', 'longitude')) %>% 
      mutate(across(c(latitude, longitude), as.numeric)) %>% 
      select(state, latitude, longitude), by = 'state')

# getting df that drops observations with NA in predictors/outcomes

nbin_df <- combined %>% 
  drop_na(abortions, births, intrastate_score, interstate_score,
          pct_bachelors, total_population, prop_hisp, prop_nonwhite,
          hh_income, dem_2party)

```

## Neighbors and Weights

States serve as the geographic units of this analysis, so I must import the pre-computed neighborhood object from GADM, which contains location information for each of the 50 states. Because this analysis is focusing on the contiguous United States, I drop Hawaii and Alaska from the observations. Then, I merge the location data with my dataframe containing the abortion data. 

I create a neighbors list using `poly2nb()` from the `spdep` package, and I build a binary weight matrix from that. Creating the neighbors in this manner will classify two states as neighbors if they share a boundary. This approach operates under the assumption that states sharing borders are most likely to share spatial characteristics not captured by the predictors included in the model. In the case of abortion spillover, it makes sense that women will cross as few state lines as possible to obtain an abortion.

For now, this analysis will adopt an approach that computes weights based on the inverse distance between states. This decision follows from a model fitting process on the three different weighting schemes. When compared to contiguity weights and inverse distance squared weights, the inverse distance weights minimized AIC in three of the four spatial models.

One could also make the case for distance-based neighborhood classifications using `dnearneigh()`, since tight clusters of states in regions such as New England likely share many regional characteristics even if they are not direct neighbors. 

I then convert the list of neighbors into a matrix where entry $W_{ij}$ contains the weight of the connection from state $i$ **to** state $j$. We build our weights matrix with binary weights such that $W_{ij}=1$ if $j$ is a neighbor of $i$ is zero otherwise. Then, I normalized the rows of the matrix such that each row of $B$ will sum to $1$ and $W_{ij} = \frac{B_{ij}}{\sum_{j} B_{ij}}.$

```{r spatial_weights}

# {
  {
    # getting spatial data for US state boundaries & subsetting out Alaska & Hawaii
    # (using tutorial at https://mhallwor.github.io/_pages/basics_SpatialPolygons)

    usa <- raster::getData('GADM', country='USA', level=1)
    usa <- usa[!usa$NAME_1 %in% c('Alaska', 'Hawaii'),]

    # merging with data

    usa <- merge(usa, nbin_df,
                 by.x = 'NAME_1', by.y = 'state',
                 duplicateGeoms = TRUE, all.x = FALSE)
    
    # ordering within-between categories so that the reference grouping makes sense
    
    usa@data$within_between <- fct_relevel(usa@data$within_between,
                                       'low-low', 'med-low', 'high-low',
                                       'low-med', 'med-med', 'high-med',
                                       'low-high', 'med-high', 'high-high')
    
  }
#   
#   # using poly2nb to create a neighbors list & from that a neighbors matrix
#   # https://rspatial.org/raster/rosu/Chapter7.html
#   
#   ### ******* I MAY WANT TO ADD A SNAP ARGUMENT IN THE FUTURE TO CONSIDER BOUNDARY
#   ### POINTS LESS THAN SNAP DISTANCE APART AS NEIGHBORS... STATES THAT DO NOT
#   ### SHARE BOUNDARIES ARE STILL CLOSE TO ONE ANOTHER AND HAVE REASONABLE
#   ### SPILLOVER / REGIONAL CORRELATION **********
#   # maybe specify neighbors with dnearneigh instead?
#   
#   usa_contig <- poly2nb(usa, queen=FALSE)
#   usa_contig
# 
# }

# writing the nb object to a file so that I don't have to recreate it every time
# (it takes forever) & can just read it in directly

#write.nb.gal(usa_contig, 'raw-data/helper/usa_contig_nb.gal')
usa_contig <- read.gal('raw-data/helper/usa_contig_nb.gal')

# check for symmetric relationships (since if TX is a neighbor of OK, OK should
# also be a neighbor of TX)

is.symmetric.nb(usa_contig)

# build binary weight matrix

weights.contig.B <- nb2listw(usa_contig, style = "B")
print(weights.contig.B)

# rescaling each row of B to sum to 1

weights.contig.W <- nb2listw(usa_contig)
image(listw2mat(weights.contig.W)[,281:1],
      axes = FALSE)

```

**it's not in kilometers, so figure out what it is**

We can also create custom weights using the inverse distance between the centroids of states. In the below code, I define $W_{ij}$ as the inverse of the distance between $i$ and $j$ in $km$ when $i$ and $j$ are neighbors, and 0 otherwise.

```{r dist_matrix}

# creating distance-based neighbors

usa.dist.range <- dnearneigh(coordinates(usa),
                            d1 = 0, # specify lower bound of range
                            d2 = 11 # specify upper bound of range
                            )
{
  # plot(usa, border="grey60", axes=FALSE, main = "Neighbors based on Distance Cutoff")
  plot(usa.dist.range, coordinates(usa), pch=19, cex=0.6)
}

# creating weights based on the inverse distance

inv.dist <- lapply(nbdists(usa.dist.range, coordinates(usa)),
                   function(x) ifelse(x!=0, 1e3/(x), x))
weights.inv.dist <- nb2listw(usa.dist.range, 
                             glist = inv.dist, # specify weights
                             style = "B")

# note style B uses the weights specified by inv.dist, 
# setting style = W will renormalize the weights to sum to one

print(weights.inv.dist)
image(listw2mat(weights.inv.dist)[,281:1],
      axes = FALSE)

# also created weights based on inverse distance squared, which will give a
# greater penalty to states with larger distances (similar to the construction
# of the interstate scores)

inv.dist2 <- lapply(nbdists(usa.dist.range, coordinates(usa)),
                   function(x) ifelse(x!=0, 1e3/(x^2), x))
weights.inv.dist2 <- nb2listw(usa.dist.range, 
                              glist = inv.dist2, # specify weights
                              style = "B")

# note style B uses the weights specified by inv.dist, 
# setting style = W will renormalize the weights to sum to one

print(weights.inv.dist2)
image(listw2mat(weights.inv.dist2)[,281:1],
      axes = FALSE)

```

```{r mod_function}

methods <- c('lm', 'wlm', 'car', 'wcar', 'sar', 'wsar')
weights_matrix <- weights.inv.dist

# writing a function to fit all of the models

make_models <- function(weights_matrix) {
  
  # creating each model object

  rate_lm <- lm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year),
              data = usa@data)
  rate_lmw <- lm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year), 
                 data = usa@data, 
                 weights = births)
  rate_car <- spautolm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_wcar <- spautolm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_sar <- spautolm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 family = 'SAR',
                 listw = weights_matrix)
  rate_wsar <- spautolm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'SAR',
                 listw = weights_matrix)
  models <- list(rate_lm, rate_lmw, rate_car, rate_wcar, rate_sar, rate_wsar)
  
  
  
  morans <- vector('list', length = length(models))
  plots <- vector('list', length = length(models))
  
  for (i in 1:length(models)) {
    
    ################
    #### MORAN
    ################
    
    {
      # conducting a Moran test on each of the models, assessing whether (p < 0.05) or
      # not (p >= 0.05) we have evidence of spatial autocorrelation not captured by
      # the model...
      # using built-in function for our lm objects, but it doesn't work on spautolm
      # objects
      
      if (class(mod) == 'lm') {
        morans[[i]] <- lm.morantest(models[[i]], listw = weights_matrix, alternative = 'two.sided')
      }
      
      # conduct a Moran test of the residuals, where p < 0.05 provides sufficient
      # evidence of additional autocorrelation not captured by the model
      
      else {
        morans[[i]] <- moran.test(residuals(models[[i]]), listw = weights_matrix, alternative = 'two.sided')
      }
    }
    
    ################
    #### PLOTS
    ################
    
    {
      # generating diagnostic plots for each model
  
      resid_vs_fit <- tibble(fit = fitted(models[[i]]), 
             resid = residuals(models[[i]])) %>% 
        ggplot(aes(fit, resid)) +
        geom_point() +
        geom_hline(yintercept = 0,
                   color = 'red',
                   linetype = 'dashed') +
        theme_minimal() +
        labs(title = 'Residuals vs. fitted values',
             x = 'Fitted values',
             y = 'Residuals')
      plot_data <- tibble(fit = fitted(models[[i]]), 
                          resid = residuals(models[[i]])) 
      qqplot <- plot_data %>% 
        ggplot(aes(sample = resid)) +
        stat_qq() +
        stat_qq_line(col = 'red') +
        theme_minimal() +
        labs(title = 'QQ-plot for normality of residuals',
             x = 'Theoretical quantiles',
             y = 'Sample quantiles')
      
      plots[[i]] <- annotate_figure(ggarrange(resid_vs_fit, qqplot),
                                    top = text_grob(paste('Diagnosic plots for', methods[[i]]),size = 14))
    }
  }
  
  # I want to return the methods, models, moran tests, and diagnostic plots
  
  return_values <- list(methods, models, morans, plots)
  return(return_values)

}

# generating the models and diagnostics for the two different weights matrices

rates_inv_dist <- make_models(weights.inv.dist)
rates_inv_dist2 <- make_models(weights.inv.dist2)
rates_contig <- make_models(weights.contig.W)

metrics <- c('AIC', 'logLik', 'lambda', 'lambda se')

get_metrics_table <- function(model_list) {
  tibble(method = rep(methods, each = length(metrics)),
       metric = rep(metrics, times = length(methods)),
       value = unlist(lapply(model_list[[2]], get_metrics))) %>% 
    pivot_wider(names_from = metric, values_from = value)
}

get_metrics_table(rates_inv_dist) %>% 
  mutate(weights = 'inv_dist') %>% 
  bind_rows(get_metrics_table(rates_inv_dist2) %>% 
              mutate(weights = 'inv_dist2')) %>% 
  bind_rows(get_metrics_table(rates_contig) %>% 
              mutate(weights = 'contig')) %>% 
  select(method, AIC, weights) %>% 
  pivot_wider(names_from = weights,
              values_from = AIC,
              names_prefix = 'AIC_') %>% 
  mutate(best_weights = case_when(AIC_inv_dist < AIC_inv_dist2 & AIC_inv_dist < AIC_contig ~ 'dist',
                                  AIC_inv_dist > AIC_inv_dist2 & AIC_inv_dist2 < AIC_contig ~ 'dist2',
                                  AIC_contig < AIC_inv_dist2 & AIC_inv_dist > AIC_contig ~ 'contig',
                                  TRUE ~ ''),
         best_weights = ifelse(best_weights == '', NA, best_weights))

```

## Testing for Spatial Autocorrelation

When testing for spatial autocorrelation in abortion rates globally, we see that the data demonstrates spatial autocorrelation at the $\alpha=0.05$ significance level.

```{r global_moran}

# moran test on row-normalized weights for contiguous neighbors

moran.test(usa$abortion_per_1k_births,
           listw = weights.contig.W)

# moran test using the inverse weighted distance

moran.test(usa$abortion_per_1k_births,
           listw = weights.inv.dist)

```

## Model fitting

### OLS

#### SHOULD I SQRT THE Y-VALUES? even though it loses its easy interpretability & the difference in the distributions is pretty negligible

```{r}
hist(usa@data$abortion_per_1k_births, 30)
hist(sqrt(usa@data$abortion_per_1k_births), 30)
```

#### INTERPRET MORAN TEST

Ignoring spatial dependence for the moment, we can fit a standard linear model

```{r rate_lm}

rate_lm <- lm(abortion_per_1k_births ~ within_between + pct_bachelors + 
                            prop_hisp + prop_nonwhite + hh_income + dem_2party + as.factor(year),
            data = usa@data)
lm.morantest(rate_lm, listw = weights.inv.dist)
plot(rate_lm)

```

We can also consider an OLS framework that accounts for heteroskedasticity in the errors by applying weights based on the number of births

```{r rate_wlm}

rate_wlm <- lm(abortion_per_1k_births ~ within_between + pct_bachelors + as.factor(year) +
                            prop_hisp + prop_nonwhite + hh_income + dem_2party,
               weights = births,
               data = usa@data)
lm.morantest(rate_wlm, listw = weights.inv.dist)
plot(rate_wlm)

```

We still see evidence of spatial autocorrelation.

I will try to adjust for this with a standard CAR model

```{r rate_car}

# had to drop a few terms so that the system was not computationally singular...
# dropped total_population

rate_car <- spautolm(abortion_per_1k_births ~ within_between + pct_bachelors + 
                       prop_hisp + as.factor(year) +
                       prop_nonwhite + hh_income + dem_2party,
               data = usa@data,
               family = 'CAR',
               listw = weights.inv.dist)

# the method automatically conducts a likelihood ratio test for the existence of
# spatial autocorrelation vs. spatial independence, which suggests the existence
# of spatial autocorrelation in this case (p-value = 2.1335e-07)

summary(rate_car)

# a Moran test of the residuals fails to find evidence of additional spatial
# autocorrelation not captured by the model (p-value = 0.7103)

moran.test(residuals(rate_car), listw = weights.inv.dist, 
           alternative = "two.sided")

# little to no deviation from normality in the residual plot

qqnorm(residuals(rate_car))
qqline(residuals(rate_car))

```

Now I will perform a weighted car, with weights for the number of births occurring in the state

```{r rate_wcar}

# copied CAR model from above but added a weights term

rate_wcar <- spautolm(abortion_per_1k_births ~ within_between + pct_bachelors + 
                        prop_hisp + as.factor(year) +
                       prop_nonwhite + hh_income + dem_2party,
               data = usa@data,
               weights = births,
               family = 'CAR',
               listw = weights.inv.dist)

# checking for evidence of spatial autocorrelation... fails to find evidence of
# spatial autocorrelation in this case (p-value = 0.679)

summary(rate_wcar)

# confirming with Moran test of residuals... finds sufficient evidence of
# additional spatial autocorrelation not captured by the model (p-value < 2.2e-16)

moran.test(residuals(rate_wcar), listw = weights.inv.dist, 
           alternative = "two.sided")

# checking residuals for deviation from normality... nothing too alarming

qqnorm(residuals(rate_wcar))
qqline(residuals(rate_wcar))

```

Now I will repeat the process with a standard SAR model, followed by a weighted SAR model

```{r rate_sar}

# had to drop a few terms so that the system was not computationally singular...
# dropped total_population

rate_sar <- spautolm(abortion_per_1k_births ~ within_between + pct_bachelors + 
                       prop_hisp + as.factor(year) +
                       prop_nonwhite + hh_income + dem_2party,
               data = usa@data,
               family = 'SAR',
               listw = weights.inv.dist)

# the method automatically conducts a likelihood ratio test for the existence of
# spatial autocorrelation vs. spatial independence, which suggests the existence
# of spatial autocorrelation in this case (p-value = 3.989e-11)

summary(rate_sar)

# a Moran test of the residuals fails to find evidence of additional spatial
# autocorrelation not captured by the model (p = 0.2841)

moran.test(residuals(rate_sar), listw = weights.inv.dist, 
           alternative = "two.sided")

# little to no deviation from normality in the residual plot

qqnorm(residuals(rate_sar))
qqline(residuals(rate_sar))

usa@data %>% 
  group_by(year) %>% 
  summarise(avg_rate = mean(abortion_per_1k_births)) %>% 
  ggplot(aes(year, avg_rate)) +
  geom_point() +
  geom_smooth()

```

Repeating the process for the weighted SAR

```{r rate_wsar}

# had to drop a few terms so that the system was not computationally singular...
# dropped total_population

rate_wsar <- spautolm(abortion_per_1k_births ~ within_between + pct_bachelors + 
                        prop_hisp + as.factor(year) +
                       prop_nonwhite + hh_income + dem_2party,
               data = usa@data,
               weights = births,
               family = 'SAR',
               listw = weights.inv.dist)

# the method automatically conducts a likelihood ratio test for the existence of
# spatial autocorrelation vs. spatial independence, which suggests the existence
# of spatial autocorrelation in this case (p-value = 3.969e-13)

summary(rate_wsar)

# a Moran test of the residuals fails to find evidence of additional spatial
# autocorrelation not captured by the model (p = 0.111)

moran.test(residuals(rate_wsar), listw = weights.inv.dist, 
           alternative = "two.sided")

# little to no deviation from normality in the residual plot

qqnorm(residuals(rate_wsar))
qqline(residuals(rate_wsar))

```

## Model Comparisons

```{r comparisons}

# creating table to compare fit of various models

get_metrics <- function(mod) {
  if (is.null(summary(mod)$lambda)) {
    metrics <- c(AIC(mod), logLik(mod), NA, NA)
  }
  else {
    metrics <- c(AIC(mod), logLik(mod), summary(mod)$lambda, summary(mod)$lambda.se)
  }
  return(metrics)
}

methods <- c('lm', 'wlm', 'car', 'wcar', 'sar', 'wsar')
metrics <- c('AIC', 'logLik', 'lambda', 'lambda se')
tibble(method = rep(methods, each = length(metrics)),
       metric = rep(metrics, times = length(methods)),
       value = c(get_metrics(rate_lm), get_metrics(rate_wlm), 
                 get_metrics(rate_car), get_metrics(rate_wcar),
                 get_metrics(rate_sar), get_metrics(rate_wsar))) %>% 
  pivot_wider(names_from = metric, values_from = value)

```

Each of the models is worsened by the use of weights based on the number of births (increased AIC and lower log-likelihood). This suggests that we should prefer the standard OLS, CAR, and SAR models over their weighted counterparts.

The unweighted models have relatively comparable AIC scores, with the SAR model performing best with the lowest AIC score, followed by the CAR model and the OLS model. Ranking the models in terms of highest log-likelihood yields the same order.

Collectively, these results suggest a standard SAR framework captures the earlier observed spatial autocorrelation in abortion rates.

## Inference on SAR Model

### DOES ACCOUNTING FOR SPATIAL AUTOCORRELATION INTRODUCE COLLINEARITY WITH MY INTERSTATE POLICY SCORES? I need to graph the confidence intervals for each of the within-between coefficients
(or I should think of other ways to visualize the within-between coefficients that also display which ones are significant)

```{r sar_inference}

# insignificant effects of the interstate scores, but significant effects of covariates

summary(rate_sar)
data.frame(coef(rate_sar)) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  arrange(coef.rate_sar.)

# this differs from the OLS model that did not account for spatial effects...
# this found *significant* NEGATIVE effects of interstate scores on abortion
# rates. that is, having stricter surroundings is associated with a decrease in
# abortion rates, even when controlling for within-state scores


```

# Other dependent variables

```{r ie_sar}

ie_sar <- spautolm(log(ie_ratio) ~ within_between + pct_bachelors + 
                       prop_hisp + 
                       prop_nonwhite + hh_income + dem_2party,
               data = usa@data,
               family = 'SAR',
               listw = weights.inv.dist)

# both p-values < 0.05... figure out what that means again

summary(ie_sar)
moran.test(residuals(ie_sar), listw = weights.inv.dist)

# getting coefficients ready to interpret... higher IE ratio indicates that more
# people travel in the state and few people leave the state.... lower ratios
# indicate that more people leave the state and fewer people come into the state
# for abortions... having the largest decrease in the IE ratio would then
# indicate that it corresponds to more people seeking abortions in neighboring
# states relative to the state in question

data.frame(coef(ie_sar)) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  mutate(coef.ie_sar. = exp(coef.ie_sar.)) %>% 
  arrange(coef.ie_sar.) %>% 
  rename(exp_coef = coef.ie_sar.)

```
log(nonres_res_ratio-1) works, but square-root is the only transformation that works for late_to_early ratio & that's not really interpretable

```{r nonres_res_sar}

# transforming the variable so that it's approximately normally distributed

hist(usa@data$nonres_res_ratio)
hist(log(usa@data$nonres_res_ratio - 1))

# fitting the model on this transformed variable

nonres_res_sar <- spautolm(log(nonres_res_ratio-1) ~ within_between + pct_bachelors + 
                       prop_hisp + 
                       prop_nonwhite + hh_income + dem_2party,
               data = usa@data,
               family = 'SAR',
               listw = weights.inv.dist)

summary(nonres_res_sar)
moran.test(residuals(nonres_res_sar), listw = weights.inv.dist)

# un-transforming the coefficients... larger nonres-res ratios indicates that a
# relatively greater share of abortions in that state come from nonresidents
# compared to residents, relative to other states... larger ratios mean
# comparatively more people travel to the state for abortions and/or residents
# leave the state for abortions

data.frame(coef(ie_sar)) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  mutate(coef.ie_sar. = exp(coef.ie_sar.)+1) %>% 
  arrange(coef.ie_sar.) %>% 
  rename(exp_coef = coef.ie_sar.)

# try fitting this on the raw data & see what happens with the residuals
# (qqplot)... if residuals still look good, we can stick with using the original
# data, but if not we can use the square root

hist(usa@data$late_to_early)
hist(sqrt(usa@data$late_to_early))

```


