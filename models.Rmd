---
title: Model Fitting on Abortion Rates
output: html_document
---

# Questions for Alex:

- how do I know that I computed the weights correctly?
  - try inverse distance squared

- need to study what the p-values mean in model summary vs moran test again

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}

knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

# packages

{
  library(MASS)
  library(tidyverse)
  library(raster)
  library(spdep)
  library(spatialreg)
  select <- dplyr::select
  library(ggpubr)
}

# data

combined <- read_csv('raw-data/combined_data.csv') %>% 
  inner_join(
    read.delim('https://www2.census.gov/geo/docs/reference/cenpop2020/CenPop2020_Mean_ST.txt') %>% 
      separate(STATEFP.STNAME.POPULATION.LATITUDE.LONGITUDE, sep = ',',
               into = c('statefip', 'state', 'population', 'latitude', 'longitude')) %>% 
      mutate(across(c(latitude, longitude), as.numeric)) %>% 
      select(state, latitude, longitude), by = 'state')

# getting df that drops observations with NA in predictors/outcomes

nbin_df <- combined %>% 
  drop_na(abortions, births, intrastate_score, interstate_score,
          pct_bachelors, total_population, prop_hisp, prop_nonwhite,
          hh_income, dem_2party)

```

## Neighbors and Weights

States serve as the geographic units of this analysis, so I must import the pre-computed neighborhood object from GADM, which contains location information for each of the 50 states. Because this analysis is focusing on the contiguous United States, I drop Hawaii and Alaska from the observations. Then, I merge the location data with my dataframe containing the abortion data. 

I create a neighbors list using `poly2nb()` from the `spdep` package, and I build a binary weight matrix from that. Creating the neighbors in this manner will classify two states as neighbors if they share a boundary. This approach operates under the assumption that states sharing borders are most likely to share spatial characteristics not captured by the predictors included in the model. In the case of abortion spillover, it makes sense that women will cross as few state lines as possible to obtain an abortion.

For now, this analysis will adopt an approach that computes weights based on the inverse distance between states. This decision follows from a model fitting process on the three different weighting schemes. When compared to contiguity weights and inverse distance squared weights, the inverse distance weights minimized AIC in three of the four spatial models.

One could also make the case for distance-based neighborhood classifications using `dnearneigh()`, since tight clusters of states in regions such as New England likely share many regional characteristics even if they are not direct neighbors. 

I then convert the list of neighbors into a matrix where entry $W_{ij}$ contains the weight of the connection from state $i$ **to** state $j$. We build our weights matrix with binary weights such that $W_{ij}=1$ if $j$ is a neighbor of $i$ is zero otherwise. Then, I normalized the rows of the matrix such that each row of $B$ will sum to $1$ and $W_{ij} = \frac{B_{ij}}{\sum_{j} B_{ij}}.$

```{r spatial_weights}

{
  {
    # getting spatial data for US state boundaries & subsetting out Alaska & Hawaii
    # (using tutorial at https://mhallwor.github.io/_pages/basics_SpatialPolygons)

    usa <- raster::getData('GADM', country='USA', level=1)
    usa <- usa[!usa$NAME_1 %in% c('Alaska', 'Hawaii'),]

    # merging with data

    usa <- merge(usa, nbin_df,
                 by.x = 'NAME_1', by.y = 'state',
                 duplicateGeoms = TRUE, all.x = FALSE)
    
    # ordering within-between categories so that the reference grouping makes sense
    
    usa@data$within_between <- fct_relevel(usa@data$within_between,
                                       'low-low', 'med-low', 'high-low',
                                       'low-med', 'med-med', 'high-med',
                                       'low-high', 'med-high', 'high-high')
    
  }

  # using poly2nb to create a neighbors list & from that a neighbors matrix
  # https://rspatial.org/raster/rosu/Chapter7.html

  ### ******* I MAY WANT TO ADD A SNAP ARGUMENT IN THE FUTURE TO CONSIDER BOUNDARY
  ### POINTS LESS THAN SNAP DISTANCE APART AS NEIGHBORS... STATES THAT DO NOT
  ### SHARE BOUNDARIES ARE STILL CLOSE TO ONE ANOTHER AND HAVE REASONABLE
  ### SPILLOVER / REGIONAL CORRELATION **********
  # maybe specify neighbors with dnearneigh instead?

  # usa_contig <- poly2nb(usa, queen=FALSE)

}

# writing the nb object to a file so that I don't have to recreate it every time
# (it takes forever) & can just read it in directly

# write.nb.gal(usa_contig, 'raw-data/helper/usa_contig_nb.gal')
usa_contig <- read.gal('raw-data/helper/usa_contig_nb.gal')

# check for symmetric relationships (since if TX is a neighbor of OK, OK should
# also be a neighbor of TX)

is.symmetric.nb(usa_contig)

# build binary weight matrix

weights.contig.B <- nb2listw(usa_contig, style = "B")
print(weights.contig.B)

# rescaling each row of B to sum to 1

weights.contig.W <- nb2listw(usa_contig)
image(listw2mat(weights.contig.W)[,281:1],
      axes = FALSE)

```

We can also create custom weights using the inverse distance between the centroids of states. Because one can easily travel across multiple state lines in regions with many small state, it makes more sense in the context of this problem to use weights computed by the inverse distance between the two states. This aligns well with the population-weighted, distance-decayed weights applied to the interstate policy scores. In the spirit of the scoring mechanism, I constructed spatial weights based on the distance and the distance squared with the intent of testing which one better captures the trends present in the data. In the below code, I define $W_{ij}$ as the inverse of the distance -- or distance squared when appropriate -- between $i$ and $j$ in $km$ when $i$ and $j$ are neighbors, and 0 otherwise.

```{r dist_matrix}

# creating weights based on the inverse distance

inv.dist <- lapply(nbdists(usa.dist.range, coordinates(usa)),
                   function(x) ifelse(x!=0, 1e3/(x), x))
weights.inv.dist <- nb2listw(usa.dist.range, 
                             glist = inv.dist, # specify weights
                             style = "B")

# note style B uses the weights specified by inv.dist, 
# setting style = W will renormalize the weights to sum to one

print(weights.inv.dist)
image(listw2mat(weights.inv.dist)[,281:1],
      axes = FALSE)

# also created weights based on inverse distance squared, which will give a
# greater penalty to states with larger distances (similar to the construction
# of the interstate scores)

inv.dist2 <- lapply(nbdists(usa.dist.range, coordinates(usa)),
                   function(x) ifelse(x!=0, 1e3/(x^2), x))
weights.inv.dist2 <- nb2listw(usa.dist.range, 
                              glist = inv.dist2, # specify weights
                              style = "B")

# note style B uses the weights specified by inv.dist, 
# setting style = W will renormalize the weights to sum to one

print(weights.inv.dist2)
image(listw2mat(weights.inv.dist2)[,281:1],
      axes = FALSE)

```

## Testing for Spatial Autocorrelation

When testing for spatial autocorrelation in abortion rates globally, we see that the data demonstrates spatial autocorrelation at the $\alpha=0.05$ significance level for all three spatial weighting schemes.

** need to figure out what this actually means for the data **

```{r global_moran}

# moran test on row-normalized weights for contiguous neighbors

moran.test(usa$abortion_per_1k_births,
           listw = weights.contig.W)

# moran test using the inverse weighted distance & distance squared

moran.test(usa$abortion_per_1k_births,
           listw = weights.inv.dist)
moran.test(usa$abortion_per_1k_births,
           listw = weights.inv.dist2)

```

## Model fitting

```{r rate_mod_function}

methods <- c('lm', 'wlm', 'car', 'wcar', 'sar', 'wsar')
weights_matrix <- weights.inv.dist

# writing a function to fit all of the models

make_models <- function(weights_matrix) {
  
  # creating each model object

  rate_lm <- lm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year),
              data = usa@data)
  rate_lmw <- lm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year), 
                 data = usa@data, 
                 weights = births)
  rate_car <- spautolm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_wcar <- spautolm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_sar <- spautolm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 family = 'SAR',
                 listw = weights_matrix)
  rate_wsar <- spautolm(abortion_per_1k_births ~ within_between + pct_bachelors + prop_hisp + 
                  prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'SAR',
                 listw = weights_matrix)
  models <- list(rate_lm, rate_lmw, rate_car, rate_wcar, rate_sar, rate_wsar)
  
  
  
  morans <- vector('list', length = length(models))
  plots <- vector('list', length = length(models))
  
  for (i in 1:length(models)) {
    
    ################
    #### MORAN
    ################
    
    {
      # conducting a Moran test on each of the models, assessing whether (p < 0.05) or
      # not (p >= 0.05) we have evidence of spatial autocorrelation not captured by
      # the model...
      # using built-in function for our lm objects, but it doesn't work on spautolm
      # objects
      
      if (class(models[[i]]) == 'lm') {
        morans[[i]] <- lm.morantest(models[[i]], listw = weights_matrix, alternative = 'two.sided')
      }
      
      # conduct a Moran test of the residuals, where p < 0.05 provides sufficient
      # evidence of additional autocorrelation not captured by the model
      
      else {
        morans[[i]] <- moran.test(residuals(models[[i]]), listw = weights_matrix, alternative = 'two.sided')
      }
    }
    
    ################
    #### PLOTS
    ################
    
    {
      # generating diagnostic plots for each model
  
      resid_vs_fit <- tibble(fit = fitted(models[[i]]), 
             resid = residuals(models[[i]])) %>% 
        ggplot(aes(fit, resid)) +
        geom_point() +
        geom_hline(yintercept = 0,
                   color = 'red',
                   linetype = 'dashed') +
        theme_minimal() +
        labs(title = 'Residuals vs. fitted values',
             x = 'Fitted values',
             y = 'Residuals')
      plot_data <- tibble(fit = fitted(models[[i]]), 
                          resid = residuals(models[[i]])) 
      qqplot <- plot_data %>% 
        ggplot(aes(sample = resid)) +
        stat_qq() +
        stat_qq_line(col = 'red') +
        theme_minimal() +
        labs(title = 'QQ-plot for normality of residuals',
             x = 'Theoretical quantiles',
             y = 'Sample quantiles')
      
      plots[[i]] <- annotate_figure(ggarrange(resid_vs_fit, qqplot),
                                    top = text_grob(paste('Diagnosic plots for', methods[[i]]),size = 14))
    }
  }
  
  # I want to return the methods, models, moran tests, and diagnostic plots
  
  return_values <- list(methods, models, morans, plots)
  return(return_values)

}

# generating the models and diagnostics for the two different weights matrices

rates_inv_dist <- make_models(weights.inv.dist)
rates_inv_dist2 <- make_models(weights.inv.dist2)
rates_contig <- make_models(weights.contig.W)

```

### OLS

#### SHOULD I SQRT THE Y-VALUES? even though it loses its easy interpretability & the difference in the distributions is pretty negligible

```{r rate_distributions}
hist(usa@data$abortion_per_1k_births, 30)
hist(sqrt(usa@data$abortion_per_1k_births), 30)


# plot to justify including year as a factor rather than linear term in the models

usa@data %>% 
  group_by(year) %>% 
  summarise(avg_rate = mean(abortion_per_1k_births)) %>% 
  ggplot(aes(year, avg_rate)) +
  geom_point() +
  geom_smooth()

```

#### INTERPRET MORAN TEST

Ignoring spatial dependence for the moment, we can fit a standard linear model

```{r rate_lm}

# printing model output from the function that created all of the models

summary(rates_inv_dist[[2]][1][[1]])

# printing moran test

rates_inv_dist[[3]][1][[1]]

# printing diagnostic plots

rates_inv_dist[[4]][1][[1]]

```

We can also consider an OLS framework that accounts for heteroskedasticity in the errors by applying weights based on the number of births

```{r rate_wlm}

# printing model output from the function that created all of the models

summary(rates_inv_dist[[2]][2][[1]])

# printing moran test

rates_inv_dist[[3]][2][[1]]

# printing diagnostic plots

rates_inv_dist[[4]][2][[1]]

```

We still see evidence of spatial autocorrelation.

I will try to adjust for this with a standard CAR model

```{r rate_car}

# printing model output from the function that created all of the models

summary(rates_inv_dist[[2]][3][[1]])

# printing moran test

rates_inv_dist[[3]][3][[1]]

# printing diagnostic plots

rates_inv_dist[[4]][3][[1]]

```

Now I will perform a weighted car, with weights for the number of births occurring in the state

```{r rate_wcar}

# printing model output from the function that created all of the models

summary(rates_inv_dist[[2]][4][[1]])

# printing moran test

rates_inv_dist[[3]][4][[1]]

# printing diagnostic plots

rates_inv_dist[[4]][4][[1]]

```

Now I will repeat the process with a standard SAR model, followed by a weighted SAR model

```{r rate_sar}

# printing model output from the function that created all of the models

summary(rates_inv_dist[[2]][5][[1]])

# printing moran test

rates_inv_dist[[3]][5][[1]]

# printing diagnostic plots

rates_inv_dist[[4]][5][[1]]

```

Repeating the process for the weighted SAR

```{r rate_wsar}

# printing model output from the function that created all of the models

summary(rates_inv_dist[[2]][6][[1]])

# printing moran test

rates_inv_dist[[3]][6][[1]]

# printing diagnostic plots

rates_inv_dist[[4]][6][[1]]

```

## Model Comparisons

```{r comparisons}

# creating table to compare fit of various models

get_metrics <- function(mod) {
  if (is.null(summary(mod)$lambda)) {
    metrics <- c(AIC(mod), logLik(mod), NA, NA)
  }
  else {
    metrics <- c(AIC(mod), logLik(mod), summary(mod)$lambda, summary(mod)$lambda.se)
  }
  return(metrics)
}

metrics <- c('AIC', 'logLik', 'lambda', 'lambda se')

get_metrics_table <- function(model_list) {
  tibble(method = rep(methods, each = length(metrics)),
       metric = rep(metrics, times = length(methods)),
       value = unlist(lapply(model_list[[2]], get_metrics))) %>% 
    pivot_wider(names_from = metric, values_from = value)
}

get_metrics_table(rates_inv_dist) %>% 
  mutate(weights = 'inv_dist') %>% 
  bind_rows(get_metrics_table(rates_inv_dist2) %>% 
              mutate(weights = 'inv_dist2')) %>% 
  bind_rows(get_metrics_table(rates_contig) %>% 
              mutate(weights = 'contig')) %>% 
  select(method, AIC, weights) %>% 
  pivot_wider(names_from = weights,
              values_from = AIC,
              names_prefix = 'AIC_') %>% 
  mutate(best_weights = case_when(AIC_inv_dist < AIC_inv_dist2 & AIC_inv_dist < AIC_contig ~ 'dist',
                                  AIC_inv_dist > AIC_inv_dist2 & AIC_inv_dist2 < AIC_contig ~ 'dist2',
                                  AIC_contig < AIC_inv_dist2 & AIC_inv_dist > AIC_contig ~ 'contig',
                                  TRUE ~ ''),
         best_weights = ifelse(best_weights == '', NA, best_weights))

```

Each of the models is worsened by the use of weights based on the number of births (increased AIC and lower log-likelihood). This suggests that we should prefer the standard OLS, CAR, and SAR models over their weighted counterparts.

The unweighted models have relatively comparable AIC scores, with the SAR model performing best with the lowest AIC score, followed by the CAR model and the OLS model. Ranking the models in terms of highest log-likelihood yields the same order.

Collectively, these results suggest a standard SAR framework captures the earlier observed spatial autocorrelation in abortion rates.

## Inference on SAR Model

### DOES ACCOUNTING FOR SPATIAL AUTOCORRELATION INTRODUCE COLLINEARITY WITH MY INTERSTATE POLICY SCORES? I need to graph the confidence intervals for each of the within-between coefficients
(or I should think of other ways to visualize the within-between coefficients that also display which ones are significant)

```{r sar_inference}

# printing out coefficients for policy categories on our chosen model

summary(rates_inv_dist[[2]][5][[1]])
data.frame(coef(rates_inv_dist[[2]][5][[1]])) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  arrange(`coef.rates_inv_dist..2...5...1...`)

```

# Other dependent variables

## Import-export ratio

```{r ie_mod_function}

methods <- c('lm', 'wlm', 'car', 'wcar', 'sar', 'wsar')
weights_matrix <- weights.inv.dist

# writing a function to fit all of the models

make_models <- function(weights_matrix) {
  
  # creating each model object

  rate_lm <- lm(log(ie_ratio) ~ within_between + pct_bachelors + prop_hisp + 
                       prop_nonwhite + hh_income + dem_2party + as.factor(year),
              data = usa@data)
  rate_lmw <- lm(log(ie_ratio) ~ within_between + pct_bachelors + prop_hisp + 
                       prop_nonwhite + hh_income + dem_2party + as.factor(year), 
                 data = usa@data, 
                 weights = births)
  rate_car <- spautolm(log(ie_ratio) ~ within_between + pct_bachelors + prop_hisp + 
                       prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_wcar <- spautolm(log(ie_ratio) ~ within_between + pct_bachelors + prop_hisp + 
                       prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_sar <- spautolm(log(ie_ratio) ~ within_between + pct_bachelors + prop_hisp + 
                       prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 family = 'SAR',
                 listw = weights_matrix)
  rate_wsar <- spautolm(log(ie_ratio) ~ within_between + pct_bachelors + prop_hisp + 
                       prop_nonwhite + hh_income + dem_2party + as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'SAR',
                 listw = weights_matrix)
  models <- list(rate_lm, rate_lmw, rate_car, rate_wcar, rate_sar, rate_wsar)
  
  
  
  morans <- vector('list', length = length(models))
  plots <- vector('list', length = length(models))
  
  for (i in 1:length(models)) {
    
    ################
    #### MORAN
    ################
    
    {
      # conducting a Moran test on each of the models, assessing whether (p < 0.05) or
      # not (p >= 0.05) we have evidence of spatial autocorrelation not captured by
      # the model...
      # using built-in function for our lm objects, but it doesn't work on spautolm
      # objects
      
      if (class(models[[i]]) == 'lm') {
        morans[[i]] <- lm.morantest(models[[i]], listw = weights_matrix, alternative = 'two.sided')
      }
      
      # conduct a Moran test of the residuals, where p < 0.05 provides sufficient
      # evidence of additional autocorrelation not captured by the model
      
      else {
        morans[[i]] <- moran.test(residuals(models[[i]]), listw = weights_matrix, alternative = 'two.sided')
      }
    }
    
    ################
    #### PLOTS
    ################
    
    {
      # generating diagnostic plots for each model
  
      resid_vs_fit <- tibble(fit = fitted(models[[i]]), 
             resid = residuals(models[[i]])) %>% 
        ggplot(aes(fit, resid)) +
        geom_point() +
        geom_hline(yintercept = 0,
                   color = 'red',
                   linetype = 'dashed') +
        theme_minimal() +
        labs(title = 'Residuals vs. fitted values',
             x = 'Fitted values',
             y = 'Residuals')
      plot_data <- tibble(fit = fitted(models[[i]]), 
                          resid = residuals(models[[i]])) 
      qqplot <- plot_data %>% 
        ggplot(aes(sample = resid)) +
        stat_qq() +
        stat_qq_line(col = 'red') +
        theme_minimal() +
        labs(title = 'QQ-plot for normality of residuals',
             x = 'Theoretical quantiles',
             y = 'Sample quantiles')
      
      plots[[i]] <- annotate_figure(ggarrange(resid_vs_fit, qqplot),
                                    top = text_grob(paste('Diagnosic plots for', methods[[i]]),size = 14))
    }
  }
  
  # I want to return the methods, models, moran tests, and diagnostic plots
  
  return_values <- list(methods, models, morans, plots)
  return(return_values)

}

# generating the models and diagnostics for the two different weights matrices

ie_inv_dist <- make_models(weights.inv.dist)
ie_inv_dist2 <- make_models(weights.inv.dist2)
ie_contig <- make_models(weights.contig.W)

```

```{r ie_comparisons}

# producing table as I did before... lowest AIC observed in SAR model with
# inv_dist weights

get_metrics_table(ie_inv_dist) %>% 
  mutate(weights = 'inv_dist') %>% 
  bind_rows(get_metrics_table(ie_inv_dist2) %>% 
              mutate(weights = 'inv_dist2')) %>% 
  bind_rows(get_metrics_table(ie_contig) %>% 
              mutate(weights = 'contig')) %>% 
  select(method, AIC, weights) %>% 
  pivot_wider(names_from = weights,
              values_from = AIC,
              names_prefix = 'AIC_') %>% 
  mutate(best_weights = case_when(AIC_inv_dist < AIC_inv_dist2 & AIC_inv_dist < AIC_contig ~ 'dist',
                                  AIC_inv_dist > AIC_inv_dist2 & AIC_inv_dist2 < AIC_contig ~ 'dist2',
                                  AIC_contig < AIC_inv_dist2 & AIC_inv_dist > AIC_contig ~ 'contig',
                                  TRUE ~ ''),
         best_weights = ifelse(best_weights == '', NA, best_weights))

```

```{r ie_sar}

# printing out coefficients for policy categories on our chosen model

ie_sar <- ie_inv_dist[[2]][5][[1]]
summary(ie_sar)
data.frame(coef(ie_sar)) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  arrange(coef.ie_sar.)

# printing moran test.. both this & the LR in the model summary have p-values <
# 0.05... figure out what that means again

ie_inv_dist[[3]][5][[1]]

# printing diagnostic plots

ie_inv_dist[[4]][5][[1]]

# getting coefficients ready to interpret... higher IE ratio indicates that more
# people travel in the state and few people leave the state.... lower ratios
# indicate that more people leave the state and fewer people come into the state
# for abortions... having the largest decrease in the IE ratio would then
# indicate that it corresponds to more people seeking abortions in neighboring
# states relative to the state in question

data.frame(coef(ie_sar)) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  mutate(coef.ie_sar. = exp(coef.ie_sar.)) %>% 
  arrange(coef.ie_sar.) %>% 
  rename(exp_coef = coef.ie_sar.)

```


## Nonresident-resident ratio

```{r nonres_mod_function}

# transforming the variable so that it's approximately normally distributed

hist(usa@data$nonres_res_ratio)
hist(log(usa@data$nonres_res_ratio - 1))

# copied code from above functions but changed the model terms to match the
# nonres-res specification

methods <- c('lm', 'wlm', 'car', 'wcar', 'sar', 'wsar')
weights_matrix <- weights.inv.dist

# writing a function to fit all of the models

make_models <- function(weights_matrix) {
  
  # creating each model object

  rate_lm <- lm(log(nonres_res_ratio-1) ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
              data = usa@data)
  rate_lmw <- lm(log(nonres_res_ratio-1) ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year), 
                 data = usa@data, 
                 weights = births)
  rate_car <- spautolm(log(nonres_res_ratio-1) ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
                 data = usa@data,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_wcar <- spautolm(log(nonres_res_ratio-1) ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_sar <- spautolm(log(nonres_res_ratio-1) ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
                 data = usa@data,
                 family = 'SAR',
                 listw = weights_matrix)
  rate_wsar <- spautolm(log(nonres_res_ratio-1) ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'SAR',
                 listw = weights_matrix)
  models <- list(rate_lm, rate_lmw, rate_car, rate_wcar, rate_sar, rate_wsar)
  
  
  
  morans <- vector('list', length = length(models))
  plots <- vector('list', length = length(models))
  
  for (i in 1:length(models)) {
    
    ################
    #### MORAN
    ################
    
    {
      # conducting a Moran test on each of the models, assessing whether (p < 0.05) or
      # not (p >= 0.05) we have evidence of spatial autocorrelation not captured by
      # the model...
      # using built-in function for our lm objects, but it doesn't work on spautolm
      # objects
      
      if (class(models[[i]]) == 'lm') {
        morans[[i]] <- lm.morantest(models[[i]], listw = weights_matrix, alternative = 'two.sided')
      }
      
      # conduct a Moran test of the residuals, where p < 0.05 provides sufficient
      # evidence of additional autocorrelation not captured by the model
      
      else {
        morans[[i]] <- moran.test(residuals(models[[i]]), listw = weights_matrix, alternative = 'two.sided')
      }
    }
    
    ################
    #### PLOTS
    ################
    
    {
      # generating diagnostic plots for each model
  
      resid_vs_fit <- tibble(fit = fitted(models[[i]]), 
             resid = residuals(models[[i]])) %>% 
        ggplot(aes(fit, resid)) +
        geom_point() +
        geom_hline(yintercept = 0,
                   color = 'red',
                   linetype = 'dashed') +
        theme_minimal() +
        labs(title = 'Residuals vs. fitted values',
             x = 'Fitted values',
             y = 'Residuals')
      plot_data <- tibble(fit = fitted(models[[i]]), 
                          resid = residuals(models[[i]])) 
      qqplot <- plot_data %>% 
        ggplot(aes(sample = resid)) +
        stat_qq() +
        stat_qq_line(col = 'red') +
        theme_minimal() +
        labs(title = 'QQ-plot for normality of residuals',
             x = 'Theoretical quantiles',
             y = 'Sample quantiles')
      
      plots[[i]] <- annotate_figure(ggarrange(resid_vs_fit, qqplot),
                                    top = text_grob(paste('Diagnosic plots for', methods[[i]]),size = 14))
    }
  }
  
  # I want to return the methods, models, moran tests, and diagnostic plots
  
  return_values <- list(methods, models, morans, plots)
  return(return_values)

}

# generating the models and diagnostics for the two different weights matrices

nonres_res_inv_dist <- make_models(weights.inv.dist)
nonres_res_inv_dist2 <- make_models(weights.inv.dist2)
nonres_res_contig <- make_models(weights.contig.W)

```

```{r nonres_res_comparisons}

# producing table as I did before... lowest AIC observed in SAR model with
# inv_dist weights

get_metrics_table(nonres_res_inv_dist) %>% 
  mutate(weights = 'inv_dist') %>% 
  bind_rows(get_metrics_table(nonres_res_inv_dist2) %>% 
              mutate(weights = 'inv_dist2')) %>% 
  bind_rows(get_metrics_table(nonres_res_contig) %>% 
              mutate(weights = 'contig')) %>% 
  select(method, AIC, weights) %>% 
  pivot_wider(names_from = weights,
              values_from = AIC,
              names_prefix = 'AIC_') %>% 
  mutate(best_weights = case_when(AIC_inv_dist < AIC_inv_dist2 & AIC_inv_dist < AIC_contig ~ 'dist',
                                  AIC_inv_dist > AIC_inv_dist2 & AIC_inv_dist2 < AIC_contig ~ 'dist2',
                                  AIC_contig < AIC_inv_dist2 & AIC_inv_dist > AIC_contig ~ 'contig',
                                  TRUE ~ ''),
         best_weights = ifelse(best_weights == '', NA, best_weights))

```

```{r nonres_res_sar}

# printing out coefficients for policy categories on our chosen model

nonres_res_sar <- nonres_res_inv_dist[[2]][5][[1]]
summary(nonres_res_sar)
data.frame(coef(nonres_res_sar)) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  arrange(coef.nonres_res_sar.)

# printing moran test.. both this & the LR in the model summary have p-values <
# 0.05... figure out what that means again

nonres_res_inv_dist[[3]][5][[1]]

# printing diagnostic plots

nonres_res_inv_dist[[4]][5][[1]]

# log(nonres/res - 1) = log((nonres-res) / res), so exponentiating coefficients
# yields the multiplicative increase in the difference between nonresident and
# resident abortions as a ratio of resident abortions... larger values means
# that there are relatively more out-of-state abortions & smaller values mean
# that there are relatively fewer out-of-state abortions

data.frame(coef(nonres_res_sar)) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  mutate(coef.nonres_res_sar. = exp(coef.nonres_res_sar.)) %>% 
  arrange(desc(coef.nonres_res_sar.)) %>% 
  rename(exp_coef = coef.nonres_res_sar.)

```

## Late-to-early ratio

```{r late_early_mod_function}

# try fitting this on the raw data & see what happens with the residuals
# (qqplot)... if residuals still look good, we can stick with using the original
# data, but if not we can use the square root

hist(usa@data$late_to_early)
hist(sqrt(usa@data$late_to_early))

# copied code from above functions but changed the model terms to match the
# nonres-res specification

methods <- c('lm', 'wlm', 'car', 'wcar', 'sar', 'wsar')
weights_matrix <- weights.inv.dist

# writing a function to fit all of the models

make_models <- function(weights_matrix) {
  
  # creating each model object

  rate_lm <- lm(late_to_early ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
              data = usa@data)
  rate_lmw <- lm(late_to_early ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year), 
                 data = usa@data, 
                 weights = births)
  rate_car <- spautolm(late_to_early ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
                 data = usa@data,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_wcar <- spautolm(late_to_early ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'CAR',
                 listw = weights_matrix)
  rate_sar <- spautolm(late_to_early ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
                 data = usa@data,
                 family = 'SAR',
                 listw = weights_matrix)
  rate_wsar <- spautolm(late_to_early ~ within_between + pct_bachelors + 
                  prop_hisp + prop_nonwhite + hh_income + dem_2party +
                  as.factor(year),
                 data = usa@data,
                 weights = births,
                 family = 'SAR',
                 listw = weights_matrix)
  models <- list(rate_lm, rate_lmw, rate_car, rate_wcar, rate_sar, rate_wsar)
  
  morans <- vector('list', length = length(models))
  plots <- vector('list', length = length(models))
  
  for (i in 1:length(models)) {
    
    ################
    #### MORAN
    ################
    
    {
      # conducting a Moran test on each of the models, assessing whether (p < 0.05) or
      # not (p >= 0.05) we have evidence of spatial autocorrelation not captured by
      # the model...
      # using built-in function for our lm objects, but it doesn't work on spautolm
      # objects
      
      if (class(models[[i]]) == 'lm') {
        morans[[i]] <- lm.morantest(models[[i]], listw = weights_matrix, alternative = 'two.sided')
      }
      
      # conduct a Moran test of the residuals, where p < 0.05 provides sufficient
      # evidence of additional autocorrelation not captured by the model
      
      else {
        morans[[i]] <- moran.test(residuals(models[[i]]), listw = weights_matrix, alternative = 'two.sided')
      }
    }
    
    ################
    #### PLOTS
    ################
    
    {
      # generating diagnostic plots for each model
  
      resid_vs_fit <- tibble(fit = fitted(models[[i]]), 
             resid = residuals(models[[i]])) %>% 
        ggplot(aes(fit, resid)) +
        geom_point() +
        geom_hline(yintercept = 0,
                   color = 'red',
                   linetype = 'dashed') +
        theme_minimal() +
        labs(title = 'Residuals vs. fitted values',
             x = 'Fitted values',
             y = 'Residuals')
      plot_data <- tibble(fit = fitted(models[[i]]), 
                          resid = residuals(models[[i]])) 
      qqplot <- plot_data %>% 
        ggplot(aes(sample = resid)) +
        stat_qq() +
        stat_qq_line(col = 'red') +
        theme_minimal() +
        labs(title = 'QQ-plot for normality of residuals',
             x = 'Theoretical quantiles',
             y = 'Sample quantiles')
      
      plots[[i]] <- annotate_figure(ggarrange(resid_vs_fit, qqplot),
                                    top = text_grob(paste('Diagnosic plots for', methods[[i]]),size = 14))
    }
  }
  
  # I want to return the methods, models, moran tests, and diagnostic plots
  
  return_values <- list(methods, models, morans, plots)
  return(return_values)

}

# generating the models and diagnostics for the two different weights matrices

late_early_inv_dist <- make_models(weights.inv.dist)
late_early_inv_dist2 <- make_models(weights.inv.dist2)
late_early_contig <- make_models(weights.contig.W)

```

```{r late_early_comparisons}

# producing table as I did before... lowest AIC observed in SAR model with
# inv_dist weights

get_metrics_table(late_early_inv_dist) %>% 
  mutate(weights = 'inv_dist') %>% 
  bind_rows(get_metrics_table(late_early_inv_dist2) %>% 
              mutate(weights = 'inv_dist2')) %>% 
  bind_rows(get_metrics_table(late_early_contig) %>% 
              mutate(weights = 'contig')) %>% 
  select(method, AIC, weights) %>% 
  pivot_wider(names_from = weights,
              values_from = AIC,
              names_prefix = 'AIC_') %>% 
  mutate(best_weights = case_when(AIC_inv_dist < AIC_inv_dist2 & AIC_inv_dist < AIC_contig ~ 'dist',
                                  AIC_inv_dist > AIC_inv_dist2 & AIC_inv_dist2 < AIC_contig ~ 'dist2',
                                  AIC_contig < AIC_inv_dist2 & AIC_inv_dist > AIC_contig ~ 'contig',
                                  TRUE ~ ''),
         best_weights = ifelse(best_weights == '', NA, best_weights))

```

# are residuals good enough? or should I square-root transform while sacrificing interpretability? I also need to update the comments for this code about the coefficient interpretations

```{r nonres_res_sar}

# printing out coefficients for policy categories on our chosen model

late_early_sar <- late_early_inv_dist[[2]][5][[1]]
summary(late_early_sar)
data.frame(coef(late_early_sar)) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  arrange(coef.late_early_sar.)

# printing moran test.. both this & the LR in the model summary have p-values <
# 0.05... figure out what that means again

late_early_inv_dist[[3]][5][[1]]

# printing diagnostic plots

late_early_inv_dist[[4]][5][[1]]

# log(nonres/res - 1) = log((nonres-res) / res), so exponentiating coefficients
# yields the multiplicative increase in the difference between nonresident and
# resident abortions as a ratio of resident abortions... larger values means
# that there are relatively more out-of-state abortions & smaller values mean
# that there are relatively fewer out-of-state abortions

data.frame(coef(late_early_sar)) %>% 
  rownames_to_column('term') %>% 
  filter(str_detect(term, 'within_between')) %>% 
  mutate(coef.late_early_sar. = exp(coef.late_early_sar.)) %>% 
  arrange(desc(coef.late_early_sar.)) %>% 
  rename(exp_coef = coef.late_early_sar.)

```

